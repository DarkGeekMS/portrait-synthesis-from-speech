In the previous section we discussed how Face Refinement can be done by changing Facial attributes. One of the important attributes is changing the yaw angle of the generated face to a specific angle. This module is designed specifically for this purpose.

\subsubsection{Functional Description}
This modules is responsible of changing the yaw pose of the generated Face by providing a specific yaw angle.

\begin{itemize}
    \item \textbf{Input :}
    \begin{itemize}
        \item 2D Image for the generated Face.
        \item Target yaw angle $\Theta$

    \end{itemize}
    \item \textbf{Output :}
    \begin{itemize}
        \item 2D Image for the generated Face rotated with angle $\Theta$
    \end{itemize}
\end{itemize}
\subsubsection{Modular Decomposition}

The Multiple Head Poses Generation is performed on multiple steps: \\

\begin{itemize}
    \item \textbf{3D Face Fitting :} The first step is generating a 3D Morphable Model (3DMM) from the input 2D Image. The resulting 3DMM is parametrized with the following parameters:
        \begin{itemize}
            \item $\overline{S}$: The mean 3D Face Mesh 
            \item $A _{exp}$: The Eigen vectors responsible for Face Expression
            \item $A _{id}$: The Eigen vectors responsible for the Face Identity
            \item $\alpha _{exp}$: The Face Expression parameters
            \item $\alpha _{id}$: The Face Identity parameters.
            \item $R$: The Euler angles representing the Face Rotation in the 3D space
            \item $t _{2d}$: Face translation matrix in the 2D space
            \item $f$: Face scale parameter in the 2D space
        \end{itemize}
        
        Thus the resulting 3D Model is represented as follows:
        
        \[ S = \overline{S} + A _{id}\alpha _{id} + A _{exp}\alpha _{exp} \]
        
        To project a 3d vertix of the model to the 2d space, The following equation is used:
        
        \[V _{2d} = f*Pr*R(\overline{S} + A _{id}\alpha _{id} + A _{exp}\alpha _{exp}) + t _{2d}\]
        
        Where $Pr$ is the projection matrix.
        
        In order to construct the 3D model the $R, alpha _{exp}, alpha _{id}, T _{2d}, f$ parameters need to be predicted from the input 2D image. We use a CNN with mobilenet architecture which we trained on AFLW dataset to regress the 3D Parameters by optimizing a parameter distance cost function which consists of the mean squared error between the actual parameters and the predicted parameters. The AFLW dataset consists of facial images and their corresponding 3DMM parameters. 
        
        At the end of the first stage we have a 3D model for the input image.
        
        \item \textbf{Rotating and Rendering the 3D model}: The second stage consists of rotating the resulted 3D model with the target angle in the 3D space and projecting it to the 2D Image. Pytorch's Neural Renderer is used to project the 3D Face Model into a 2D image. The texture information for the 3D model is taken from the input 2D Image by projecting each vertix into the 2D space and getting the corresponding pixel value.
        
        At the end of this stage we have a 2D Image that has the face model rotated and projected. But the 2D Image for the Face has missing parts which don't exist in the 3D Model such as the hair and teeth. In addition to the occluded parts of the face in the input 2D Image. 
        
        \item \textbf{Face Inpainting}: In this stage, Face Inpainting is performed for the resulted 2D Image from the previous step to fill the missing gaps. The model for Face Inpainting is CycleGAN which takes an Input as the Image with gaps and predicts the final 2D Image with the complete face. The CycleGAN model is trained on a generated dataset which is generated by: 
        \begin{itemize}
            \item Generating 3D Model for the face.
            \item Rotating the 3D Model with a random angle.
            \item Project the 3D Model to 2D space.
            \item Generating 3D Model for the 2D Image generated from the previous step.
            \item Rotating back the Image with the previous angle to the initial pose
            \item Projecting the Image to 2D Space yielding a pair of complete face image and face image with gaps.
        \end{itemize}
        
        At the end of this stage, The result will be a 2D Image consists of the input face rotated with the input yaw angle.

\end{itemize}

\subsubsection{Design Constraints}
In order to design a Pose Generation system suitable for practice and to be integrated with the final application, Some Design constrained has been taken.

\begin{itemize}
    \item \textbf{Models sizes}: Models sizes in this module were constrained to be small to fit into our deployment server which consists of 11 GB Nvidia 1080TI GPU and 16GB ram.
    
    \item \textbf{Models speed}: Models speed in this module were constrained to be fast so it can be run on CPU. So we made use of MobileNet CNN architecture which is lightweight and fast when it is run on CPU.
    
    \item \textbf{Output Image Size}: As GAN-based models can take around one month of training on a single GPU when it is trained on images of the 1080x1080 Resolution, We made the output image of our module to be 256x256 in order to make the training feasible given our limited computational power and to make the module fits in memory with the other modules in the system. 
    
\end{itemize}
